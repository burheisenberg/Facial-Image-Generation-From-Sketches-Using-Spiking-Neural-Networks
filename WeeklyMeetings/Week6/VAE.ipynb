{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40bec1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch running on  cuda .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch running on \" , DEVICE , \".\\n\")\n",
    "\n",
    "# Define VAE architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, num_channels, height, width, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 8, kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*height*width, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(64, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_size,latent_size)\n",
    "        )\n",
    "        \n",
    "        self.logvar = nn.Sequential(\n",
    "            nn.Linear(64, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_size,latent_size)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_size,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,int(height*width)),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (4,int(height/2),int(width/2) ) ),\n",
    "            nn.Upsample(size=(height,width),mode='bilinear',align_corners=True),\n",
    "            nn.Conv2d(4, 16, kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=(2*height,2*width),mode='bilinear',align_corners=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, num_channels, kernel_size=3,stride=1,padding=1)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mean(x)\n",
    "        logvar = self.logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        x_recon = self.decoder(z)\n",
    "        \n",
    "        # Normalize along the 3rd and 4th dimensions (height and width)\n",
    "        #min_values, _ = torch.min(x_recon, dim=2, keepdim=True)\n",
    "        #min_values, _ = torch.min(min_values, dim=3, keepdim=True)\n",
    "\n",
    "        #max_values, _ = torch.max(x_recon, dim=2, keepdim=True)\n",
    "        #max_values, _ = torch.max(max_values, dim=3, keepdim=True)\n",
    "\n",
    "        #x_recon = (x_recon - min_values) / (max_values - min_values)\n",
    "        \n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Training function\n",
    "def train_vae(model, dataloader, num_epochs=10, learning_rate=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, targets in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs = inputs.to(device)  # Ignore labels from ImageFolder\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_batch, mu, logvar = model(inputs)\n",
    "            loss = vae_loss(recon_batch, targets, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "# VAE loss function\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    #BCE = F.binary_cross_entropy(recon_x,x,reduction=\"sum\")\n",
    "    \n",
    "    # Reconstruction loss\n",
    "    MSE = F.mse_loss(recon_x, x)\n",
    "    #recons_loss = torch.amax(torch.abs(input_img-recons_img),dim=1).mean()\n",
    "    #MAE = F.l1_loss(recon_x, x)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + 1e-6 * KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430ac107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_vae(model, dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_batch, mu, logvar = model(inputs)\n",
    "\n",
    "            # Visualize original and reconstructed images\n",
    "            visualize_images(inputs, recon_batch, recon_batch.shape[0])\n",
    "\n",
    "# Visualization function\n",
    "def visualize_images(original, reconstructed, num_images=5):\n",
    "    original = original.cpu().numpy()\n",
    "    reconstructed = reconstructed.cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(num_images * 2, 4))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Original images\n",
    "        \n",
    "        reconstructed[i] = (reconstructed[i] - np.min(reconstructed[i])) / (np.max(reconstructed[i]) - np.min(reconstructed[i]))\n",
    "        \n",
    "        axes[0, i].imshow(np.transpose(original[i], (1, 2, 0)))\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "\n",
    "        # Reconstructed images\n",
    "        axes[1, i].imshow(np.transpose(reconstructed[i], (1, 2, 0)))\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(\"Reconstructed\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb110ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_folder, output_folder, transform=None):\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        self.input_images = os.listdir(input_folder)\n",
    "        self.output_images = os.listdir(output_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.input_images), len(self.output_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_folder, self.input_images[idx])\n",
    "        output_path = os.path.join(self.output_folder, self.output_images[idx])\n",
    "\n",
    "        input_image = Image.open(input_path)\n",
    "        output_image = Image.open(output_path) #.convert('RGB')  # Convert to grayscale if necessary\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            output_image = self.transform(output_image)\n",
    "\n",
    "        return input_image, output_image\n",
    "\n",
    "def get_image_dataloader(input_folder, output_folder, batch_size=64, validation_split=0.1, random_seed=42):\n",
    "    # Ensure input and output folders exist\n",
    "    #assert os.path.exists(input_folder), f\"Input folder '{input_folder}' does not exist.\"\n",
    "    #assert os.path.exists(output_folder), f\"Output folder '{output_folder}' does not exist.\"\n",
    "\n",
    "    # Define data transformations\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # Create datasets\n",
    "    dataset = CustomDataset(input_folder,output_folder, transform=transform)\n",
    "    dataset = torch.utils.data.Subset(dataset, range(20000))\n",
    "\n",
    "    # Check if the number of samples in input and output datasets match\n",
    "    #assert len(input_dataset) == len(output_dataset), \"Input and output datasets must have the same number of samples.\"\n",
    "\n",
    "    # Randomly split the datasets into training and validation sets\n",
    "    total_samples = len(dataset)\n",
    "    validation_size = int(validation_split * total_samples)\n",
    "    train_size = total_samples - validation_size\n",
    "\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "\n",
    "    # Create DataLoader instances\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101e659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e31d9d29ac4c219570a11310e041a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef742aaa42f4beeb1d3a7dc931d2581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/200:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.1306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2761d9e71d46448668304b8b0057fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/200:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200, Loss: 0.0825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942c2c00b43f49d888298fa585357aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/200:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200, Loss: 0.0818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3eee69b7c184ae5a399171ad6ae7d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/200:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200, Loss: 0.0817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818f78a74f674ea4a5230663231330d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/200:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set your data path and other parameters\n",
    "    input_image_path = \"C:/Users/Brhan/OneDrive/Belgeler/KuOnline/Spring24/Elec491/SNN/dataset/img_align_celeba_sketch\"\n",
    "    output_image_path= \"C:/Users/Brhan/OneDrive/Belgeler/KuOnline/Spring24/Elec491/SNN/dataset/img_align_celeba\"\n",
    "    \n",
    "    batch_size = 100\n",
    "    num_channels = 3  # Adjust based on your data\n",
    "    height = 218\n",
    "    width = 178\n",
    "    latent_size = 20  # Adjust based on your desired latent space dimension\n",
    "\n",
    "    # Initialize VAE\n",
    "    vae_model = VAE(num_channels=num_channels, height = height, width = width, latent_size=latent_size)\n",
    "\n",
    "    # Get data loader\n",
    "    train_loader, val_loader = get_image_dataloader(input_image_path,output_image_path,batch_size)\n",
    "\n",
    "    train=1\n",
    "    if train:\n",
    "        # Train VAE\n",
    "        train_vae(vae_model, train_loader, num_epochs=200)\n",
    "    \n",
    "    \n",
    "    # Instantiate the Convolutional VAE model and optimizer\n",
    "    #conv_vae = ConvVAE(in_channels=1, out_channels=3,latent_size=latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Data')\n",
    "test_vae(vae_model,train_loader)\n",
    "print('Validation Data')\n",
    "test_vae(vae_model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07042a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model weights\n",
    "save_params= 1\n",
    "if save_params:\n",
    "    torch.save(vae_model.state_dict(), \"g2c_100epochs_MSE.pth\")\n",
    "    torch.save(vae_model, \"model_g2c_100epochs_MSE.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f6f054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Flatten(start_dim=1, end_dim=-1)\n",
       "    (3): Linear(in_features=310432, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       "  (mean): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=20, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  )\n",
       "  (logvar): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=20, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): Linear(in_features=20, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=38804, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Unflatten(dim=1, unflattened_size=(4, 109, 89))\n",
       "    (6): Upsample(size=(218, 178), mode=bilinear)\n",
       "    (7): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): Upsample(size=(436, 356), mode=bilinear)\n",
       "    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model.load_state_dict(torch.load(\"g2c_100epochs_MSE.pth\"))\n",
    "vae_model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0326e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
