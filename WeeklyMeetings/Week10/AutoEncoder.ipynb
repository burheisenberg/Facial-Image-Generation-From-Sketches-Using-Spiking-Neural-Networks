{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oKG2sgwikhm"
   },
   "source": [
    "# Coming from TensorFlow to NengoDL\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nengo/nengo-dl/blob/master/docs/examples/from-tensorflow.ipynb)\n",
    "\n",
    "NengoDL combines two frameworks: Nengo and TensorFlow.  This tutorial is designed for\n",
    "people who are familiar with TensorFlow and looking to learn more about neuromorphic\n",
    "modelling with NengoDL.  For the other approach, users familiar with Nengo looking to\n",
    "learn how to use NengoDL, check out [this\n",
    "tutorial](https://www.nengo.ai/nengo-dl/examples/from-nengo.html).\n",
    "\n",
    "If you are familiar with Keras you may also be interested in\n",
    "[KerasSpiking](https://www.nengo.ai/keras-spiking/), a companion project to NengoDL\n",
    "that has a more minimal feature set, but integrates even more transparently with the\n",
    "Keras API. See [this page](https://www.nengo.ai/keras-spiking/nengo-dl-comparison.html)\n",
    "for a more detailed comparison between the two projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "JWh0R7ADikhr",
    "outputId": "577d7f70-c7e7-450b-ef54-39c2df65e292"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo\n",
    "from nengo.utils.matplotlib import rasterplot\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "import nengo_dl\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "tf.get_logger().addFilter(lambda rec: \"Tracing is expensive\" not in rec.msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e47sgSArikht"
   },
   "source": [
    "## What is Nengo\n",
    "\n",
    "We'll start with the very basics, where you might be wondering what Nengo is and why you\n",
    "would want to use it.  Nengo is a tool for constructing and simulating neural networks.\n",
    "That is, to some extent, the same purpose as TensorFlow (and its higher level API,\n",
    "Keras).  For example, here is how we might build a simple two layer auto-encoder network\n",
    "in Nengo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 784\n",
    "n_hidden = 64\n",
    "minibatch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "class VAE(tf.keras.Model):\n",
    "    latent_dim = 20\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(28, 28, 1)),\n",
    "            layers.Conv2D(32, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2D(64, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Flatten()\n",
    "        ])\n",
    "        \n",
    "        self.bottleneck_mean = tf.keras.Sequential([\n",
    "            layers.Input(shape=(64*7*7)),\n",
    "            layers.Dense(self.latent_dim, activation='relu') # assuming 20-dimensional latent space\n",
    "        ])\n",
    "        \n",
    "        self.bottleneck_logvar = tf.keras.Sequential([\n",
    "            layers.Input(shape=(64*7*7)),\n",
    "            layers.Dense(self.latent_dim,activation='relu')\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(self.latent_dim,)),\n",
    "            layers.Dense(7*7*64, activation='relu'),\n",
    "            layers.Reshape((7, 7, 64)),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2D(1, 3, activation='sigmoid', padding='same')\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        z_mean, z_logvar = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, z_mean, z_logvar\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        batch_size = tf.shape(mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, tf.shape(mean)[1]))\n",
    "        return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "    \n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean = self.bottleneck_mean(x)\n",
    "        logvar = self.bottleneck_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    # Training step\n",
    "    def train_step(self,data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_logvar = self.encode(inputs)\n",
    "            z = self.reparameterize(z_mean,z_logvar)\n",
    "            reconstructed = self.decode(z)\n",
    "            reconstruction_loss = tf.reduce_sum(binary_crossentropy(targets, reconstructed)*28*28, axis=[1,2])\n",
    "            kl_loss = -0.5*tf.reduce_sum(1 + z_logvar - tf.square(z_mean) - tf.exp(z_logvar), axis=-1)\n",
    "            loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Log the loss\n",
    "        loss_dict = {'loss': loss}\n",
    "        return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11828\\3781759133.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnengo_dl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nengo_dl\\converter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, allow_fallback, inference_only, max_to_avg_pool, split_shared_weights, swap_activations, scale_firing_rates, synapse, temporal_model)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;31m# convert model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m# set model from the converter in case the converter has changed the model type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nengo_dl\\converter.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Using TensorNode %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m         \u001b[0minput_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_input_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    957\u001b[0m         output = self.tensor_layer(\n\u001b[0;32m    958\u001b[0m             \u001b[0minput_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nengo_dl\\converter.py\u001b[0m in \u001b[0;36mget_input_obj\u001b[1;34m(self, node_id, tensor_idx)\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mNengo\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mwhose\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0minput\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0minput_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0minput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "converter = nengo_dl.Converter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7YGvZydrikhv"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'TensorNode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11828\\650223455.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# reparameterize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbottleneck_mean\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbottleneck_logvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# decoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'TensorNode'"
     ]
    }
   ],
   "source": [
    "# assuming you have a VAE instance\n",
    "vae = VAE()\n",
    "\n",
    "with nengo.Network() as net:\n",
    "    # assuming input images are 28x28 pixels\n",
    "    input_node = nengo.Node(output=np.zeros(28 * 28))\n",
    "\n",
    "    # create layers to match VAE structure\n",
    "    # note: you'll need to replace this with actual NengoDL layers\n",
    "    encoder = nengo_dl.Layer(tf.keras.layers.Dense(units=64, activation=tf.nn.relu))(input_node)\n",
    "    bottleneck_mean = nengo_dl.Layer(tf.keras.layers.Dense(units=vae.latent_dim, activation=tf.nn.relu))(encoder)\n",
    "    bottleneck_logvar = nengo_dl.Layer(tf.keras.layers.Dense(units=vae.latent_dim, activation=tf.nn.relu))(encoder)\n",
    "\n",
    "    # reparameterize\n",
    "    epsilon = np.random.normal(size=(vae.latent_dim,))\n",
    "    z = bottleneck_mean + tf.exp(0.5 * bottleneck_logvar) * epsilon\n",
    "\n",
    "    # decoder\n",
    "    decoder = nengo_dl.Layer(tf.keras.layers.Dense(units=7*7*64, activation=tf.nn.relu))(z)\n",
    "    output = nengo_dl.Layer(tf.keras.layers.Dense(units=28*28, activation=tf.nn.sigmoid))(decoder)\n",
    "\n",
    "# convert the Nengo network into a NengoDL simulator\n",
    "sim = nengo_dl.Simulator(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf9ABKqvikhv"
   },
   "source": [
    "One difference you'll note is that with Nengo we separate the creation of the layers and\n",
    "the creation of the connections between layers.  This is because the connection\n",
    "structure in Nengo networks often has a lot more state and general complexity than in\n",
    "typical deep learning networks, so it is helpful to be able to control it independently\n",
    "(we'll see examples of this later).\n",
    "\n",
    "Another new object you may notice is the `nengo.Probe`.  This is used to collect data\n",
    "from the simulation; by adding a probe to `nengo_c.neurons`, we are indicating that we\n",
    "want to collect the activities of those neurons when the simulation is running.  You can\n",
    "think of this like the `outputs` arguments in a Keras Model.\n",
    "\n",
    "We will not go into a lot of detail on Nengo here; there is much more functionality\n",
    "available, but we will focus on the features most familiar or relevant to those coming\n",
    "from a TensorFlow background.  For a more in-depth introduction to Nengo, check out the\n",
    "Nengo-specific [documentation](https://www.nengo.ai/nengo/) and\n",
    "[examples](https://www.nengo.ai/nengo/examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9Crcteikhz"
   },
   "source": [
    "## Inserting TensorFlow code\n",
    "\n",
    "The goal of NengoDL is not to replace TensorFlow or Nengo, but to allow them to smoothly\n",
    "work together.  Thus one important feature is the ability to write TensorFlow code\n",
    "directly, and insert it into a Nengo network.  This allows us to use whichever framework\n",
    "is best suited for different parts of a model.\n",
    "\n",
    "This functionality is accessed through the `nengo_dl.TensorNode` class.  This allows us\n",
    "to wrap TensorFlow code in a Nengo object, so that it can easily communicate with the\n",
    "rest of a Nengo model.  The TensorFlow code is written in a function that takes\n",
    "`tf.Tensors` as input, applies the desired manipulations through TensorFlow operations,\n",
    "and returns a `tf.Tensor`.  We then pass that function to the TensorNode.\n",
    "\n",
    "For simple cases we can use `nengo_dl.Layer`.  This is a simplified interface for\n",
    "constructing `TensorNodes` that mimics the Keras functional API.  For example, suppose\n",
    "we want to apply batch normalization to the output of one of the Nengo ensembles.  There\n",
    "is no built-in way to do batch normalization in Nengo, so we can instead turn to\n",
    "TensorFlow for this part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "w1J7tbT_ikh1"
   },
   "outputs": [],
   "source": [
    "with nengo.Network() as vae_net:\n",
    "    # input\n",
    "    nengo_a = nengo.Node(np.zeros(n_in))\n",
    "\n",
    "    # encoder layer\n",
    "    nengo_b = nengo.Ensemble(n_hidden, 1, neuron_type=nengo.RectifiedLinear())\n",
    "    nengo.Connection(nengo_a, nengo_b.neurons, transform=nengo_dl.dists.He())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # decoder layer\n",
    "    nengo_c = nengo.Ensemble(n_in, 1, neuron_type=nengo.RectifiedLinear())\n",
    "    nengo.Connection(\n",
    "        nengo_b.neurons, nengo_c.neurons, transform=nengo_dl.dists.He()\n",
    "    )\n",
    "\n",
    "    # probes are used to collect data from the network\n",
    "    p_c = nengo.Probe(nengo_c.neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDIyNKfEikh1"
   },
   "source": [
    "More details on TensorNode usage can be found in [the user\n",
    "guide](https://www.nengo.ai/nengo-dl/tensor-node.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRekPWmwikh1"
   },
   "source": [
    "## Deep learning parameter optimization\n",
    "\n",
    "NengoDL allows model parameters to be optimized via TensorFlow optimization algorithms,\n",
    "through the `Simulator.fit` function.  Returning to the autoencoder examples from the\n",
    "beginning of this tutorial, we'll optimize those networks to encode MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "c28EdWE3ikh1"
   },
   "outputs": [],
   "source": [
    "# download MNIST dataset\n",
    "(train_data, _), (test_data, _) = tf.keras.datasets.mnist.load_data()\n",
    "# flatten images\n",
    "train_data = train_data.reshape((train_data.shape[0], -1))\n",
    "test_data = test_data.reshape((test_data.shape[0], -1))\n",
    "\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHATU-aFikh1"
   },
   "source": [
    "Before running the same training in NengoDL, we'll change the Nengo model parameters to\n",
    "more closely match the TensorFlow network (we omitted these details in the original\n",
    "presentation to keep things simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BG2co-Qaikh2"
   },
   "outputs": [],
   "source": [
    "# set initial neuron gains to 1 and biases to 0\n",
    "for ens in vae_net.all_ensembles:\n",
    "    ens.gain = nengo.dists.Choice([1])\n",
    "    ens.bias = nengo.dists.Choice([0])\n",
    "\n",
    "# disable synaptic filtering on all connections\n",
    "for conn in vae_net.all_connections:\n",
    "    conn.synapse = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kZ_m5MGikh2"
   },
   "source": [
    "We also need to modify the data slightly.  As mentioned above, NengoDL simulations are\n",
    "essentially temporal, so data is described over time (indicating what the inputs/targets\n",
    "should be on each simulation timestep).  So instead of the data having shape\n",
    "`(batch_size, n)`, it will have shape `(batch_size, n_steps, n)`.  In this case we'll\n",
    "just be training for a single timestep, but we still need to add that extra axis with\n",
    "length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ASYzCA1Kikh2"
   },
   "outputs": [],
   "source": [
    "train_data = train_data[:, None, :]\n",
    "test_data = test_data[:, None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euh0alvfikh2"
   },
   "source": [
    "Now we can run the NengoDL equivalent of the above TensorFlow training (note: the\n",
    "results will not match exactly due to different random initializations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hbu5BMgJikh2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "Epoch 1/2\n",
      "1200/1200 [==============================] - 8s 6ms/step - loss: 1678.4037 - probe_loss: 1678.4037\n",
      "Epoch 2/2\n",
      "1200/1200 [==============================] - 7s 6ms/step - loss: 1083.7755 - probe_loss: 1083.7755\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 998.5309 - probe_loss: 998.5309\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 212ms/step                         \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbXUlEQVR4nO3db3BU15nn8V8joA2k1baMpW4ZWdEwIvYgloyBABr+CGZQrKlQxiRb2N7JiqrEZcfALiV7XSFULVReIJezpnihmFRcGQwbiKnZwX+qYIyVBYm4MC6ZhYHBxJGDMPIiRYtiq4WMGyTOvmDpdVsgfNrdetTS91N1q+h778N9OL7mp0PfPh1wzjkBAGBglHUDAICRixACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAmdHWDXzR1atXdf78eYVCIQUCAet2AACenHPq7u5WYWGhRo0aeK4z5ELo/PnzKioqsm4DAPAVtba2atKkSQOeM+RCKBQKSZLm6e81WmOMuwEA+OrVFb2lfYm/zweSsRB64YUX9LOf/UxtbW2aOnWqtmzZovnz59+y7vo/wY3WGI0OEEIAkHX+34qkX+YtlYw8mLB7926tXbtW69ev17FjxzR//nxVVVXp3LlzmbgcACBLZSSENm/erB/84Af64Q9/qPvuu09btmxRUVGRtm7dmonLAQCyVNpD6PLlyzp69KgqKyuT9ldWVurw4cP9zo/H44rFYkkbAGBkSHsIXbhwQX19fSooKEjaX1BQoPb29n7n19bWKhwOJzaejAOAkSNjH1b94htSzrkbvkm1bt06dXV1JbbW1tZMtQQAGGLS/nTcxIkTlZOT02/W09HR0W92JEnBYFDBYDDdbQAAskDaZ0Jjx47VjBkzVF9fn7S/vr5e5eXl6b4cACCLZeRzQjU1Nfr+97+vmTNnau7cufrlL3+pc+fO6YknnsjE5QAAWSojIbRixQp1dnbqpz/9qdra2lRWVqZ9+/apuLg4E5cDAGSpgHPOWTfxebFYTOFwWBV6kBUTACAL9boratBr6urqUm5u7oDn8lUOAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMJP2ENq4caMCgUDSFolE0n0ZAMAwMDoTv+nUqVP129/+NvE6JycnE5cBAGS5jITQ6NGjmf0AAG4pI+8JNTc3q7CwUCUlJXr44Yd15syZm54bj8cVi8WSNgDAyJD2EJo9e7Z27Nih/fv368UXX1R7e7vKy8vV2dl5w/Nra2sVDocTW1FRUbpbAgAMUQHnnMvkBXp6ejR58mQ988wzqqmp6Xc8Ho8rHo8nXsdiMRUVFalCD2p0YEwmWwMAZECvu6IGvaauri7l5uYOeG5G3hP6vAkTJmjatGlqbm6+4fFgMKhgMJjpNgAAQ1DGPycUj8d1+vRpRaPRTF8KAJBl0h5CTz/9tBobG9XS0qJ33nlH3/ve9xSLxVRdXZ3uSwEAslza/znuo48+0iOPPKILFy7orrvu0pw5c3TkyBEVFxen+1IAgCyX9hB6+eWX0/1bIsNGld2bWl3XRe+a3taPUroWgOGJteMAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYyfiX2mFwnXl2rnfNqJKelK41duzA35h4I73Hyr1rxrf7f/nv2IupfWFw3uHz3jVujP//RhenTvSuCTX5L/569Q7//0aSpNH+P59euf02/8t0x2990hcE+vz/2149/p53DQYHMyEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlW0R7CPtg8x7tm+swPvGv+duLvvWsk6WDnN7xr/vdc/1WT5xWc8a75Xftk7xpJOr9ivHdNV5d/jboD3iXuO4XeNaGJqa2Qfs/tH3vXfHLJfxzuHPepd82Vvhzvmo93+K8uL0l3bH87pTp8ecyEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmGEB0yEs94/+PyP8sbXUu+b45L/wrpGksZ3+C0kG/+x/nX8unehdU7r6Hf8LSWqum+1f5PxLJkzq9q5ZVNTsXTM75L/4qyT9Y+vfeNcEc/q8a6ry/827ZsZtZ71r/mHaKu8aSbojpSr4YCYEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADAuYDmH5Pz/sXTN60t3eNX3RPO8aSdKx094lrrfXu+auRfd71/T+7QzvGkma8lKPd82o7s+8az6d7L805sGps7xrGi/710jSHX+44l3T+m3/BW0jX+/yrvnjlXzvmttPB7xrMDiYCQEAzBBCAAAz3iF06NAhLV26VIWFhQoEAnr11VeTjjvntHHjRhUWFmrcuHGqqKjQqVOn0tUvAGAY8Q6hnp4eTZ8+XXV1dTc8/txzz2nz5s2qq6tTU1OTIpGIlixZou5u/y/xAgAMb94PJlRVVamqquqGx5xz2rJli9avX6/ly5dLkrZv366CggLt2rVLjz/++FfrFgAwrKT1PaGWlha1t7ersrIysS8YDGrhwoU6fPjGT3rF43HFYrGkDQAwMqQ1hNrb2yVJBQUFSfsLCgoSx76otrZW4XA4sRUVFaWzJQDAEJaRp+MCgeRn8p1z/fZdt27dOnV1dSW21tbWTLQEABiC0vph1UgkIunajCgajSb2d3R09JsdXRcMBhUMBtPZBgAgS6R1JlRSUqJIJKL6+vrEvsuXL6uxsVHl5eXpvBQAYBjwngldvHhRH3zwQeJ1S0uLjh8/rry8PN1zzz1au3atNm3apNLSUpWWlmrTpk0aP368Hn300bQ2DgDIft4h9O6772rRokWJ1zU1NZKk6upqvfTSS3rmmWd06dIlPfnkk/r44481e/ZsvfnmmwqFQunrGgAwLAScc866ic+LxWIKh8Oq0IMaHRhj3Q4wIuXc6b+o7e//6xTvmv+57L951/yXc8u8a7rnX/CuQep63RU16DV1dXUpNzd3wHNZOw4AYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYCat36wKYJgomOhdsmj2v3nX/K94oXfNyUOl3jVfF6toD1XMhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJhhAVMMS6PGj0+pLjDuNu+avs4/p3StwTD6bv8FQiXpvf90u3fNrHGnvGv+e9tc75rJL/3Ju6bPuwKDhZkQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAMyxgipSNCoW8a652d3vXjC4p9q5x3Re9aySp70Knd01OQb7/hS5f8S4JhP3H+89zU1vAtPL+f/WuWfC133vX/NP/WOhdU9R82LsGQxczIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGZYwBQpS2Ux0lT0tnw4KNeRpFETJnjX9P2pw7sm575S/+t87Tbvmk++kdrPmR/E7vKuebLlP3jXRI73etdgeGEmBAAwQwgBAMx4h9ChQ4e0dOlSFRYWKhAI6NVXX006vnLlSgUCgaRtzpw56eoXADCMeIdQT0+Ppk+frrq6upue88ADD6itrS2x7du37ys1CQAYnrwfTKiqqlJVVdWA5wSDQUUikZSbAgCMDBl5T6ihoUH5+fmaMmWKHnvsMXV03PzpoXg8rlgslrQBAEaGtIdQVVWVdu7cqQMHDuj5559XU1OTFi9erHg8fsPza2trFQ6HE1tRUVG6WwIADFFp/5zQihUrEr8uKyvTzJkzVVxcrL1792r58uX9zl+3bp1qamoSr2OxGEEEACNExj+sGo1GVVxcrObm5hseDwaDCgaDmW4DADAEZfxzQp2dnWptbVU0Gs30pQAAWcZ7JnTx4kV98MEHidctLS06fvy48vLylJeXp40bN+q73/2uotGozp49q5/85CeaOHGiHnroobQ2DgDIft4h9O6772rRokWJ19ffz6murtbWrVt18uRJ7dixQ5988omi0agWLVqk3bt3KxQKpa9rAMCw4B1CFRUVcs7d9Pj+/fu/UkNAWgQCqZWNGZw1fftO3/g90oGM+uZfedd8VnjFu0aS7rytx7um861J3jXjG05611z1rsBQxtpxAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzg7NkMDDIcu4rTa2w7f+kt5GbSGVF7POLbveuqfmbvd41krTr3EzvmtvP+K/YfbW727sGwwszIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGZYwBRDXs7EO71r+t77QwY6SZ+er3/Nuyb4d/6Lq779yV9410jSlT353jW5e99O6VoY2ZgJAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMMMCphhUOXfc4V3Td6EzA52kT2DGVO+aUU92eNfMv/Ocd80rb8/yrpGk0hdZjBSDg5kQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAMyxgCgXGjE2t7q8me9f0/evplK41WALBoHdN8/dD3jX/UHDcu6bxT6XeNZP/6bJ3DTCYmAkBAMwQQgAAM14hVFtbq1mzZikUCik/P1/Lli3T+++/n3SOc04bN25UYWGhxo0bp4qKCp06dSqtTQMAhgevEGpsbNSqVat05MgR1dfXq7e3V5WVlerp6Umc89xzz2nz5s2qq6tTU1OTIpGIlixZou7u7rQ3DwDIbl4PJrzxxhtJr7dt26b8/HwdPXpUCxYskHNOW7Zs0fr167V8+XJJ0vbt21VQUKBdu3bp8ccfT1/nAICs95XeE+rq6pIk5eXlSZJaWlrU3t6uysrKxDnBYFALFy7U4cOHb/h7xONxxWKxpA0AMDKkHELOOdXU1GjevHkqKyuTJLW3t0uSCgoKks4tKChIHPui2tpahcPhxFZUVJRqSwCALJNyCK1evVonTpzQb37zm37HAoFA0mvnXL99161bt05dXV2JrbW1NdWWAABZJqUPq65Zs0avv/66Dh06pEmTJiX2RyIRSddmRNFoNLG/o6Oj3+zoumAwqGAKHxAEAGQ/r5mQc06rV6/Wnj17dODAAZWUlCQdLykpUSQSUX19fWLf5cuX1djYqPLy8vR0DAAYNrxmQqtWrdKuXbv02muvKRQKJd7nCYfDGjdunAKBgNauXatNmzaptLRUpaWl2rRpk8aPH69HH300I38AAED28gqhrVu3SpIqKiqS9m/btk0rV66UJD3zzDO6dOmSnnzySX388ceaPXu23nzzTYVC/utrAQCGt4Bzzlk38XmxWEzhcFgVelCjA2Os28EAcnJzvWv6UvnQciq36E0ehMnEtRaeuORdc1X+/b30ZoV3TemGk941knT1cx9AB3z1uitq0Gvq6upS7i3+nmDtOACAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAmZS+WRWQpL5YzLqFmxr17+5Nqe73T37Nu2ZG3zveNf/8h2961/zlb/xXIGc1bAx1zIQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYYQFTDHk5t4e9a1oevCOla1X+9THvmqN/7f+z3PhV/guluqOHvWuAoY6ZEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADMsYIoh72LFN7xriis+TOlaTxX81rum+l/+o3dNcOdV7xpgOGImBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwLmGLI+2iR/89K/1j8Lyld64rzv1Zu1R+9a3oX3+5dAwxHzIQAAGYIIQCAGa8Qqq2t1axZsxQKhZSfn69ly5bp/fffTzpn5cqVCgQCSducOXPS2jQAYHjwCqHGxkatWrVKR44cUX19vXp7e1VZWamenp6k8x544AG1tbUltn379qW1aQDA8OD1YMIbb7yR9Hrbtm3Kz8/X0aNHtWDBgsT+YDCoSCSSng4BAMPWV3pPqKurS5KUl5eXtL+hoUH5+fmaMmWKHnvsMXV0dNz094jH44rFYkkbAGBkSDmEnHOqqanRvHnzVFZWlthfVVWlnTt36sCBA3r++efV1NSkxYsXKx6P3/D3qa2tVTgcTmxFRUWptgQAyDIpf05o9erVOnHihN56662k/StWrEj8uqysTDNnzlRxcbH27t2r5cuX9/t91q1bp5qamsTrWCxGEAHACJFSCK1Zs0avv/66Dh06pEmTJg14bjQaVXFxsZqbm294PBgMKhgMptIGACDLeYWQc05r1qzRK6+8ooaGBpWUlNyyprOzU62trYpGoyk3CQAYnrzeE1q1apV+/etfa9euXQqFQmpvb1d7e7suXbokSbp48aKefvppvf322zp79qwaGhq0dOlSTZw4UQ899FBG/gAAgOzlNRPaunWrJKmioiJp/7Zt27Ry5Url5OTo5MmT2rFjhz755BNFo1EtWrRIu3fvVigUSlvTAIDhwfuf4wYybtw47d+//ys1BAAYOVhFG0Ne6X8+4l1T8e+vpnStbxfOTanO1+gDRwflOsBQxwKmAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzLCAKYalbxd+07oFAF8CMyEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmBlya8c55yRJvboiOeNmAADeenVF0v//+3wgQy6Euru7JUlvaZ9xJwCAr6K7u1vhcHjAcwLuy0TVILp69arOnz+vUCikQCCQdCwWi6moqEitra3Kzc016tAe43AN43AN43AN43DNUBgH55y6u7tVWFioUaMGftdnyM2ERo0apUmTJg14Tm5u7oi+ya5jHK5hHK5hHK5hHK6xHodbzYCu48EEAIAZQggAYCarQigYDGrDhg0KBoPWrZhiHK5hHK5hHK5hHK7JtnEYcg8mAABGjqyaCQEAhhdCCABghhACAJghhAAAZrIqhF544QWVlJTotttu04wZM/S73/3OuqVBtXHjRgUCgaQtEolYt5Vxhw4d0tKlS1VYWKhAIKBXX3016bhzThs3blRhYaHGjRuniooKnTp1yqbZDLrVOKxcubLf/TFnzhybZjOktrZWs2bNUigUUn5+vpYtW6b3338/6ZyRcD98mXHIlvsha0Jo9+7dWrt2rdavX69jx45p/vz5qqqq0rlz56xbG1RTp05VW1tbYjt58qR1SxnX09Oj6dOnq66u7obHn3vuOW3evFl1dXVqampSJBLRkiVLEusQDhe3GgdJeuCBB5Luj337htcajI2NjVq1apWOHDmi+vp69fb2qrKyUj09PYlzRsL98GXGQcqS+8FliW9961vuiSeeSNp37733uh//+MdGHQ2+DRs2uOnTp1u3YUqSe+WVVxKvr1696iKRiHv22WcT+z777DMXDofdL37xC4MOB8cXx8E556qrq92DDz5o0o+Vjo4OJ8k1NjY650bu/fDFcXAue+6HrJgJXb58WUePHlVlZWXS/srKSh0+fNioKxvNzc0qLCxUSUmJHn74YZ05c8a6JVMtLS1qb29PujeCwaAWLlw44u4NSWpoaFB+fr6mTJmixx57TB0dHdYtZVRXV5ckKS8vT9LIvR++OA7XZcP9kBUhdOHCBfX19amgoCBpf0FBgdrb2426GnyzZ8/Wjh07tH//fr344otqb29XeXm5Ojs7rVszc/2//0i/NySpqqpKO3fu1IEDB/T888+rqalJixcvVjwet24tI5xzqqmp0bx581RWViZpZN4PNxoHKXvuhyG3ivZAvvjVDs65fvuGs6qqqsSvp02bprlz52ry5Mnavn27ampqDDuzN9LvDUlasWJF4tdlZWWaOXOmiouLtXfvXi1fvtyws8xYvXq1Tpw4obfeeqvfsZF0P9xsHLLlfsiKmdDEiROVk5PT7yeZjo6Ofj/xjCQTJkzQtGnT1NzcbN2KmetPB3Jv9BeNRlVcXDws7481a9bo9ddf18GDB5O++mWk3Q83G4cbGar3Q1aE0NixYzVjxgzV19cn7a+vr1d5eblRV/bi8bhOnz6taDRq3YqZkpISRSKRpHvj8uXLamxsHNH3hiR1dnaqtbV1WN0fzjmtXr1ae/bs0YEDB1RSUpJ0fKTcD7cahxsZsveD4UMRXl5++WU3ZswY96tf/cq99957bu3atW7ChAnu7Nmz1q0Nmqeeeso1NDS4M2fOuCNHjrjvfOc7LhQKDfsx6O7udseOHXPHjh1zktzmzZvdsWPH3Icffuicc+7ZZ5914XDY7dmzx508edI98sgjLhqNulgsZtx5eg00Dt3d3e6pp55yhw8fdi0tLe7gwYNu7ty57u677x5W4/CjH/3IhcNh19DQ4Nra2hLbp59+mjhnJNwPtxqHbLofsiaEnHPu5z//uSsuLnZjx451999/f9LjiCPBihUrXDQadWPGjHGFhYVu+fLl7tSpU9ZtZdzBgwedpH5bdXW1c+7aY7kbNmxwkUjEBYNBt2DBAnfy5EnbpjNgoHH49NNPXWVlpbvrrrvcmDFj3D333OOqq6vduXPnrNtOqxv9+SW5bdu2Jc4ZCffDrcYhm+4HvsoBAGAmK94TAgAMT4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMz8X5QuAFHnKRztAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with nengo_dl.Simulator(vae_net, minibatch_size=minibatch_size) as sim:\n",
    "    sim.compile(optimizer=tf.optimizers.RMSprop(1e-3), loss=tf.losses.mse)\n",
    "\n",
    "    # run training loop\n",
    "    sim.fit(train_data, train_data, epochs=n_epochs)\n",
    "\n",
    "    # evaluate performance on test set\n",
    "    sim.evaluate(test_data, test_data)\n",
    "\n",
    "    # display example output\n",
    "    plt.figure()\n",
    "    plt.imshow(test_data[0,0,:].reshape((28, 28)))\n",
    "    plt.show()\n",
    "    output = sim.predict(test_data[:minibatch_size])\n",
    "    plt.figure()\n",
    "    plt.imshow(output[p_c][0].reshape((28, 28)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Probe of 'output' of <Neurons of <Ensemble (unlabeled) at 0x16d31a66f08>>>\n",
      "{<Probe at 0x16d6c241688 of 'output' of <Neurons of <Ensemble (unlabeled) at 0x16d31a66f08>>>: array([[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(p_c)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzFEtKXnikh2"
   },
   "source": [
    "More details on using `sim.fit` can be found in [the user\n",
    "guide](https://www.nengo.ai/nengo-dl/simulator.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq8DjylXikiA"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we have demonstrated how to translate TensorFlow concepts into NengoDL,\n",
    "including network construction, execution, and optimization.  We have also discussed how\n",
    "to use TensorNodes to combine TensorFlow and Nengo code, and introduced some of the\n",
    "unique features of Nengo (such as NEF optimization and neuromorphic cross-platform\n",
    "execution).  However, there is much more functionality in NengoDL than we are able to\n",
    "introduce here; check out the [user\n",
    "guide](https://www.nengo.ai/nengo-dl/user-guide.html) or [other\n",
    "examples](https://www.nengo.ai/nengo-dl/examples.html) for more information.  If you\n",
    "would like more information on how NengoDL is implemented under the hood using\n",
    "TensorFlow, check out the [white paper](https://arxiv.org/abs/1805.11144)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
